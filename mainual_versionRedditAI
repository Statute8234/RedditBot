import openpyxl
import praw, random
import pandas as pd
from sklearn.model_selection import train_test_split
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Embedding, LSTM, Dense, Input, concatenate
from keras.models import Model
from keras.optimizers import Adam
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
import numpy as np
from datetime import datetime
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from keras.models import Sequential
# path
path = r"C:\Users\danie\OneDrive\Desktop\Data.xlsx"
workbook = openpyxl.load_workbook(path)
worksheet = workbook.active
# reddit
reddit = praw.Reddit(
    client_id="nY4WstIZvogjrDxmIWyKPw",
    client_secret="jKZGZhgITUi8rd_xgMtlkscMx62JoA",
    user_agent="test-script",
)
# --- add data
current_row = 2
subreddit_name = "randomscreenshot"
subreddit = reddit.subreddit(subreddit_name)
num_posts = subreddit.subscribers
print(f"Looking through {num_posts} posts....")
# Get existing titles from the Excel file
existing_titles = set(worksheet.cell(row=row, column=1).value for row in range(2, worksheet.max_row + 1))
for submission in reddit.subreddit(subreddit_name).new(limit=None):
    if submission.title not in existing_titles:  # Check if the title already exists
        worksheet = workbook['Sheet1']
        worksheet.cell(row=current_row, column=1, value=submission.title)
        worksheet.cell(row=current_row, column=2, value=submission.score)
        worksheet.cell(row=current_row, column=3, value=submission.num_comments)
        worksheet.cell(row=current_row, column=4, value=len(submission.all_awardings))
        created_utc_timestamp = submission.created_utc
        created_utc_datetime = datetime.utcfromtimestamp(created_utc_timestamp)
        formatted_timestamp = datetime.utcfromtimestamp(created_utc_timestamp).strftime('%Y-%m-%d %H:%M:%S')
        worksheet.cell(row=current_row, column=5, value=formatted_timestamp)
        worksheet.cell(row=current_row, column=6, value=submission.url)
        worksheet.cell(row=current_row, column=7, value=submission.link_flair_text)
        workbook.save(path)
        current_row += 1
workbook.close()
# preprocess text
def preprocess_text(text):
    words = word_tokenize(text)
    filtered_words = [ps.stem(w.lower()) for w in words if w.isalpha() and w.lower() not in stopwords and w not in string.punctuation and not w.isdigit()]
    return " ".join(filtered_words)
# build model
def build_model(max_words, max_text_length):
    model = Sequential()
    model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_text_length))
    model.add(LSTM(100))
    model.add(Dense(1, activation='sigmoid'))  # Linear activation for regression
    model.compile(optimizer='SGD', loss='huber_loss')
    return model
# read data
data = pd.read_excel(path)
max_text_length = 100
tokenizer = Tokenizer()
tokenizer.fit_on_texts(data['Title'])
X_text = pad_sequences(tokenizer.texts_to_sequences(data['Title']), maxlen=max_text_length)
X_text = pad_sequences(X_text, maxlen=max_text_length)
X_numerical = data[['Score', 'Comments', 'Awards']].values
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(data['Title'])
# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data, y_encoded, test_size=0.5, random_state=42, shuffle=True)
X_train_text = pad_sequences(tokenizer.texts_to_sequences(X_train['Title']), maxlen=max_text_length)
# Text input model
max_words = len(tokenizer.word_index) + 1
embedding_dim = 128
text_input = Input(shape=(max_text_length,))
text_embedding = Embedding(input_dim=max_words, output_dim=embedding_dim)(text_input)
text_lstm = LSTM(100)(text_embedding)
# Numerical input model
numerical_input = Input(shape=(3,))
numerical_dense = Dense(100)(numerical_input)
# Merge models
merged = concatenate([text_lstm, numerical_dense])
output = Dense(max_words, activation='softmax')(merged) 
model = Model(inputs=[text_input, numerical_input], outputs=output)
model.compile(optimizer=Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit([X_text, X_numerical], y_encoded, epochs=10, batch_size=64, validation_split=0.2)
# User input and prediction
user_input_text = "Man I wish I had multiple arms"
user_input_text_seq = tokenizer.texts_to_sequences([user_input_text])
user_input_text_padded = pad_sequences(user_input_text_seq, maxlen=max_text_length)
user_input_numerical = np.array([[random.randint(0, 100), random.randint(0, 100), random.randint(0, 100)]])
predicted_word_index = np.argmax(model.predict([user_input_text_padded, user_input_numerical]))
predicted_word = [word for word, index in tokenizer.word_index.items() if index == predicted_word_index][0]
# Build models for score, comments, and awards
score_model = build_model(max_words, max_text_length)
comments_model = build_model(max_words, max_text_length)
awards_model = build_model(max_words, max_text_length)
X_train_text = pad_sequences(tokenizer.texts_to_sequences(X_train['Title']), maxlen=max_text_length)
# Train and predict for score, comments, and awards
score_model.fit(X_train_text, X_train['Score'].values, batch_size=64, epochs=10, validation_split=0.5)
comments_model.fit(X_train_text, X_train['Comments'].values, batch_size=64, epochs=10, validation_split=0.5)
awards_model.fit(X_train_text, X_train['Awards'].values, batch_size=64, epochs=10, validation_split=0.5)
# Predict for user input
user_input_processed = preprocess_text(predicted_word)
user_input_sequence = tokenizer.texts_to_sequences([user_input_processed])
user_input_padded = pad_sequences(user_input_sequence, maxlen=max_text_length)
avg_score = round(np.mean(score_model.predict(user_input_padded)))
avg_comments = round(np.mean(comments_model.predict(user_input_padded)))
avg_awards = round(np.mean(awards_model.predict(user_input_padded)))
print(f"Predicted Score: {avg_score}")
print(f"Predicted Comments: {avg_comments}")
print(f"Predicted Awards: {avg_awards}")
