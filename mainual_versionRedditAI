import openpyxl, praw, random,string
import pandas as pd
from sklearn.model_selection import train_test_split
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Embedding, LSTM, Dense, Input, concatenate
from keras.models import Model
from keras.optimizers import Adam
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
import numpy as np
from datetime import datetime
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from keras.models import Sequential
stopwords = nltk.corpus.stopwords.words('english')
avrigeTitleSize = []
# path
path = r"C:\Users\danie\OneDrive\Desktop\Data.xlsx"
workbook = openpyxl.load_workbook(path)
worksheet = workbook.active
# reddit
reddit = praw.Reddit(
    client_id="nY4WstIZvogjrDxmIWyKPw",
    client_secret="jKZGZhgITUi8rd_xgMtlkscMx62JoA",
    user_agent="test-script",
)
# --- add data
current_row = 2
subreddit_name = "randomscreenshot"
subreddit = reddit.subreddit(subreddit_name)
num_posts = subreddit.subscribers
print(f"Looking through {num_posts} posts....")
# Get existing titles from the Excel file
existing_titles = set(worksheet.cell(row=row, column=1).value for row in range(2, worksheet.max_row + 1))
for submission in reddit.subreddit(subreddit_name).new(limit=None):
    if submission.title not in existing_titles:  # Check if the title already exists
        worksheet = workbook['Sheet1']
        worksheet.cell(row=current_row, column=1, value=submission.title)
        avrigeTitleSize.append(len(submission.title))
        worksheet.cell(row=current_row, column=2, value=submission.score)
        worksheet.cell(row=current_row, column=3, value=submission.num_comments)
        worksheet.cell(row=current_row, column=4, value=len(submission.all_awardings))
        created_utc_timestamp = submission.created_utc
        created_utc_datetime = datetime.utcfromtimestamp(created_utc_timestamp)
        formatted_timestamp = datetime.utcfromtimestamp(created_utc_timestamp).strftime('%Y-%m-%d %H:%M:%S')
        worksheet.cell(row=current_row, column=5, value=formatted_timestamp)
        worksheet.cell(row=current_row, column=6, value=submission.url)
        worksheet.cell(row=current_row, column=7, value=submission.link_flair_text)
        workbook.save(path)
        current_row += 1
workbook.close()
# preprocess text
ps = PorterStemmer()
def preprocess_text(text):
    words = word_tokenize(text)
    filtered_words = [ps.stem(w.lower()) for w in words if w.isalpha() and w.lower() not in stopwords and w not in string.punctuation and not w.isdigit()]
    return " ".join(filtered_words)
# build model
def build_model(max_words, max_text_length):
    model = Sequential()
    model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_text_length))
    model.add(LSTM(100))
    model.add(Dense(1, activation='sigmoid'))  # Linear activation for regression
    model.compile(optimizer='SGD', loss='huber_loss')
    return model
# Function to preprocess user input and get predictions
def get_predictions(user_input, tokenizer, max_text_length, score_model, comments_model, awards_model):
    processed_input = preprocess_text(user_input)
    # Tokenize and pad the input text
    input_sequence = tokenizer.texts_to_sequences([processed_input])
    input_sequence = pad_sequences(input_sequence, maxlen=max_text_length)
    # Get predictions for score, comments, and awards
    score_prediction = score_model.predict(input_sequence)
    comments_prediction = comments_model.predict(input_sequence)
    awards_prediction = awards_model.predict(input_sequence)
    # Calculate average predictions
    avg_score = round(np.mean(score_prediction))
    avg_comments = round(np.mean(comments_prediction))
    avg_awards = round(np.mean(awards_prediction))
    # Calculate standard deviation for each prediction
    std_score = np.std(score_prediction)
    std_comments = np.std(comments_prediction)
    std_awards = np.std(awards_prediction)
    # Calculate confidence interval (assuming normal distribution)
    confidence_level = 0.95
    z_score = 1.96
    confidence_interval_score = (avg_score - z_score * std_score, avg_score + z_score * std_score)
    confidence_interval_comments = (avg_comments - z_score * std_comments, avg_comments + z_score * std_comments)
    confidence_interval_awards = (avg_awards - z_score * std_awards, avg_awards + z_score * std_awards)
    return avg_score, confidence_interval_score, avg_comments, confidence_interval_comments, avg_awards, confidence_interval_awards
# read data
data = pd.read_excel(path)
max_text_length = 100
tokenizer = Tokenizer()
tokenizer.fit_on_texts(data['Title'])
X_text = pad_sequences(tokenizer.texts_to_sequences(data['Title']), maxlen=max_text_length)
X_text = pad_sequences(X_text, maxlen=max_text_length)
X_numerical = data[['Score', 'Comments', 'Awards']].values
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(data['Title'])
# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data, y_encoded, test_size=0.5, random_state=50, shuffle=True)
X_train_text = pad_sequences(tokenizer.texts_to_sequences(X_train['Title']), maxlen=max_text_length)
# Text input model
max_words = len(tokenizer.word_index) + 1
embedding_dim = 128
text_input = Input(shape=(max_text_length,))
text_embedding = Embedding(input_dim=max_words, output_dim=embedding_dim)(text_input)
text_lstm = LSTM(100)(text_embedding)
# Numerical input model
numerical_input = Input(shape=(3,))
numerical_dense = Dense(100)(numerical_input)
# Merge models
merged = concatenate([text_lstm, numerical_dense])
output = Dense(max_words, activation='softmax')(merged) 
model = Model(inputs=[text_input, numerical_input], outputs=output)
model.compile(optimizer=Adam(lr=0.1), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit([X_text, X_numerical], y_encoded, epochs=10, batch_size=64, validation_split=0.2)
# User input and prediction
user_input_text = "Man I wish I had multiple arms"
user_input_text_seq = tokenizer.texts_to_sequences([user_input_text])
user_input_text_padded = pad_sequences(user_input_text_seq, maxlen=max_text_length)
user_input_numerical = np.array([[random.randint(0, 100), random.randint(0, 100), random.randint(0, 100)]])
predicted_word_index = np.argmax(model.predict([user_input_text_padded, user_input_numerical]))
predicted_word = [word for word, index in tokenizer.word_index.items() if index == predicted_word_index][0]
# Build models for score, comments, and awards
score_model = build_model(max_words, max_text_length)
comments_model = build_model(max_words, max_text_length)
awards_model = build_model(max_words, max_text_length)
X_train_text = pad_sequences(tokenizer.texts_to_sequences(X_train['Title']), maxlen=max_text_length)
# Train and predict for score, comments, and awards
score_model.fit(X_train_text, X_train['Score'].values, batch_size=64, epochs=10, validation_split=0.5)
comments_model.fit(X_train_text, X_train['Comments'].values, batch_size=64, epochs=10, validation_split=0.5)
awards_model.fit(X_train_text, X_train['Awards'].values, batch_size=64, epochs=10, validation_split=0.5)
# Predict for user input
TItlelist = []
avrigeTitleSize = sum(avrigeTitleSize)//len(avrigeTitleSize)
for _ in range(round(random.triangular(1,avrigeTitleSize,5))):
    user_input_text_seq = tokenizer.texts_to_sequences([user_input_text])
    user_input_text_padded = pad_sequences(user_input_text_seq, maxlen=max_text_length)
    user_input_numerical = np.array([[random.randint(0, 100), random.randint(0, 100), random.randint(0, 100)]])
    predicted_word_index = np.argmax(model.predict([user_input_text_padded, user_input_numerical]))
    predicted_word = [word for word, index in tokenizer.word_index.items() if index == predicted_word_index][0]
    # ---
    user_input_processed = preprocess_text(predicted_word)
    user_input_sequence = tokenizer.texts_to_sequences([user_input_processed])
    user_input_padded = pad_sequences(user_input_sequence, maxlen=max_text_length)
    TItlelist.append(predicted_word)
NewTitle = ' '.join(TItlelist)
print("title:",NewTitle)
avg_score, ci_score, avg_comments, ci_comments, avg_awards, ci_awards = get_predictions(NewTitle, tokenizer, max_text_length, score_model, comments_model, awards_model)
print(f"Predicted Score: {avg_score} (Confidence Interval: {ci_score})")
print(f"Predicted Comments: {avg_comments} (Confidence Interval: {ci_comments})")
print(f"Predicted Awards: {avg_awards} (Confidence Interval: {ci_awards})")
